@book{aiebook2025,
  address   = {USA},
  author    = {Chip Huyen},
  isbn      = {978-1801819312},
  publisher = {O'Reilly Media},
  title     = {{AI Engineering}},
  year      = {2025}
}

@techreport{forrester2022,
  author      = {{Forrester Consulting}},
  title       = {The Crisis of Fractured Organizations: How Teams Can Address Organizational Misalignment \& Achieve More In The Modern Work Environment},
  institution = {Forrester Research, Inc.},
  type        = {Thought Leadership Paper},
  date        = {2022-12},
  note        = {Commissioned by Airtable},
  url         = {https://www.airtable.com/lp/resources/reports/crisis-of-the-fractured-organization}
}

@misc{hu2023decipher,
  title         = {DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4},
  author        = {Yebowen Hu and Kaiqiang Song and Sangwoo Cho and Xiaoyang Wang and Hassan Foroosh and Fei Liu},
  year          = {2023},
  eprint        = {2305.14702},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2305.14702}
}

@misc{ke2025demystifyingdomainadaptiveposttrainingfinancial,
  title         = {Demystifying Domain-adaptive Post-training for Financial LLMs},
  author        = {Zixuan Ke and Yifei Ming and Xuan-Phi Nguyen and Caiming Xiong and Shafiq Joty},
  year          = {2025},
  eprint        = {2501.04961},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2501.04961}
}

@misc{krishnan2025aiagentsevolutionarchitecture,
  title         = {AI Agents: Evolution, Architecture, and Real-World Applications},
  author        = {Naveen Krishnan},
  year          = {2025},
  eprint        = {2503.12687},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2503.12687}
}

@misc{lu2025bizfinbench,
  title         = {BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs},
  author        = {Guilong Lu and Xuntao Guo and Rongjunchen Zhang and Wenqiao Zhu and Ji Liu},
  year          = {2025},
  eprint        = {2505.19457},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2505.19457}
}

@inproceedings{tanabe2024enhancingfinancialdomainadaptation,
  title     = {Enhancing Financial Domain Adaptation of Language Models via Model Augmentation},
  url       = {http://dx.doi.org/10.1109/BigData62323.2024.10825292},
  doi       = {10.1109/bigdata62323.2024.10825292},
  booktitle = {2024 IEEE International Conference on Big Data (BigData)},
  publisher = {IEEE},
  author    = {Tanabe, Kota and Hirano, Masanori and Matoya, Kazuki and Imajo, Kentaro and Sakaji, Hiroki and Noda, Itsuki},
  year      = {2024},
  month     = dec,
  pages     = {6661–6669}
}


@article{wang2025replacehumanevaluators,
  title     = {Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering},
  volume    = {2},
  issn      = {2994-970X},
  url       = {http://dx.doi.org/10.1145/3728963},
  doi       = {10.1145/3728963},
  number    = {ISSTA},
  journal   = {Proceedings of the ACM on Software Engineering},
  publisher = {Association for Computing Machinery (ACM)},
  author    = {Wang, Ruiqi and Guo, Jiyu and Gao, Cuiyun and Fan, Guodong and Chong, Chun Yong and Xia, Xin},
  year      = {2025},
  month     = jun,
  pages     = {1955–1977}
}

@misc{zhuge2024agentasajudgeevaluateagentsagents,
  title         = {Agent-as-a-Judge: Evaluate Agents with Agents},
  author        = {Mingchen Zhuge and Changsheng Zhao and Dylan Ashley and Wenyi Wang and Dmitrii Khizbullin and Yunyang Xiong and Zechun Liu and Ernie Chang and Raghuraman Krishnamoorthi and Yuandong Tian and Yangyang Shi and Vikas Chandra and Jürgen Schmidhuber},
  year          = {2024},
  eprint        = {2410.10934},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/2410.10934}
}

@article{llmcausalreasoning2024,
  author         = {Wang, Lei and Shen, Yiqing},
  title          = {Evaluating Causal Reasoning Capabilities of Large Language Models: A Systematic Analysis Across Three Scenarios},
  journal        = {Electronics},
  volume         = {13},
  year           = {2024},
  number         = {23},
  article-number = {4584},
  url            = {https://www.mdpi.com/2079-9292/13/23/4584},
  issn           = {2079-9292},
  abstract       = {Large language models (LLMs) have shown their capabilities in numerical and logical reasoning, yet their capabilities in higher-order cognitive tasks, particularly causal reasoning, remain less explored. Current research on LLMs in causal reasoning has focused primarily on tasks such as identifying simple cause-effect relationships, answering basic “what-if” questions, and generating plausible causal explanations. However, these models often struggle with complex causal structures, confounding variables, and distinguishing correlation from causation. This work addresses these limitations by systematically evaluating LLMs’ causal reasoning abilities across three representative scenarios, namely analyzing causation from effects, tracing effects back to causes, and assessing the impact of interventions on causal relationships. These scenarios are designed to challenge LLMs beyond simple associative reasoning and test their ability to handle more nuanced causal problems. For each scenario, we construct four paradigms and employ three types of prompt scheme, namely zero-shot prompting, few-shot prompting, and Chain-of-Thought (CoT) prompting in a set of 36 test cases. Our findings reveal that most LLMs encounter challenges in causal cognition across all prompt schemes, which underscore the need to enhance the cognitive reasoning capabilities of LLMs to better support complex causal reasoning tasks. By identifying these limitations, our study contributes to guiding future research and development efforts in improving LLMs’ higher-order reasoning abilities.},
  doi            = {10.3390/electronics13234584}
}
